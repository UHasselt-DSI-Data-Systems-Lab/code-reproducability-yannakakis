use std::{
    collections::{HashMap, HashSet},
    fs::File,
    path::{Path, PathBuf},
    sync::Arc,
};

use datafusion::{
    arrow::{
        array::RecordBatch,
        datatypes::{Field, Schema},
    },
    execution::context::SessionState,
    parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder,
    physical_plan::ExecutionPlan,
    prelude::{ParquetReadOptions, SessionContext},
};
use yannakakis_join_implementation::{
    display::ExtDisplayableExecutionPlan, yannakakis::flatten::Flatten,
};

use crate::intermediate_plan::{self, Plan};

/// Read parquet file into a vector of [RecordBatch] of size `batch_size`.
///
/// # Panics
/// Panics if the file cannot be found/read or if there are no batches in the file.
pub fn read_parquet(
    path: &PathBuf,
    batch_size: usize,
) -> Result<Vec<RecordBatch>, datafusion::common::DataFusionError> {
    let file = File::open(path)?;
    let builder = ParquetRecordBatchReaderBuilder::try_new(file)?;
    let builder = builder.with_batch_size(batch_size);
    //    let schema = builder.schema().clone();
    let mut reader = builder.build()?;
    let mut batches = vec![];
    while let Some(batch) = reader.next() {
        batches.push(batch?);
    }
    println!("Read {} batches from {:?}", batches.len(), path);
    Ok(batches)
}

/// Register all parquet files in a directory as in-memory tables.
/// Returns a map from table name to (path, schema, record batches).
pub fn read_parquets_in_dir(
    directory: &Path,
    batch_size: usize,
) -> HashMap<String, (PathBuf, Arc<Schema>, Vec<RecordBatch>)> {
    let mut tables = HashMap::new();

    // Read all parquet files in directory
    let files = std::fs::read_dir(directory).unwrap();
    for file in files {
        let file = file.unwrap();
        // Skip non-parquet files
        if file.path().extension().unwrap() != "parquet" {
            continue;
        }
        let batches = read_parquet(&file.path(), batch_size).unwrap();
        let schema = batches[0].schema();
        // table & field names to lowercase
        let schema = Arc::new(Schema::new(
            schema
                .fields()
                .iter()
                .map(|field| {
                    Arc::new(Field::new(
                        field.name().to_lowercase(),
                        field.data_type().clone(),
                        field.is_nullable(),
                    ))
                })
                .collect::<Vec<_>>(),
        ));
        let table_name = file
            .path()
            .file_stem()
            .unwrap()
            .to_str()
            .unwrap()
            .to_string()
            .to_lowercase();
        tables.insert(table_name.into(), (file.path(), schema, batches));
    }

    tables
}

/// Returns a JSON string containing the metrics of the given [ExecutionPlan].
pub fn metrics(plan: &Arc<dyn ExecutionPlan>) -> String {
    //use std::fmt::Write;

    // collect metrics generated by DataFusion
    let metrics = ExtDisplayableExecutionPlan::with_full_metrics(plan.as_ref())
        .json(true)
        .to_string();

    metrics
}

/// Returns a JSON string (without newlines) containing the detailed metrics of Yannakakis in the given [ExecutionPlan].
/// It searches top-down for the first [Flatten] or [Unnest] node and returns its detailed metrics.
/// Returns None if no [Flatten] or [Unnest] node is found.
pub fn yann_detailed_metrics(plan: &Arc<dyn ExecutionPlan>) -> Option<String> {
    match plan.as_any().downcast_ref::<Flatten>() {
        Some(flatten) => Some(
            flatten
                .as_json()
                .expect("Error while writing flatten metrics to json string."),
        ),
        None => {
            for child in plan.children() {
                let metrics = yann_detailed_metrics(&child);
                if metrics.is_some() {
                    return metrics;
                }
            }
            return None;
        }
    }
}

/// A wrapper around DataFusion's [SessionContext] that also:
/// 1) Stores the schema and [RecordBatch]es for each relation.
/// 2) Stores aliases for relations (must be updated before running a plan using [Catalog::update()]).
pub struct Catalog {
    /// Stores (schema, batches) for each relation
    relations: HashMap<String, (Arc<Schema>, Vec<RecordBatch>)>,
    /// DataFusion session context
    ctx: SessionContext,
    /// Map aliases to relation names
    alias_to_rel: HashMap<String, String>,
    /// Map relation names to aliases
    rel_to_alias: HashMap<String, HashSet<String>>,
    /// All relations in the current plan (original names, not aliases)
    rels_in_plan: HashSet<String>,
}

impl Catalog {
    #[allow(dead_code)]
    pub fn new() -> Self {
        Catalog {
            relations: HashMap::new(),
            ctx: SessionContext::new(),
            alias_to_rel: HashMap::new(),
            rel_to_alias: HashMap::new(),
            rels_in_plan: HashSet::new(),
        }
    }

    #[allow(dead_code)]
    pub fn new_with_context(ctx: SessionContext) -> Self {
        Catalog {
            relations: HashMap::new(),
            ctx,
            alias_to_rel: HashMap::new(),
            rel_to_alias: HashMap::new(),
            rels_in_plan: HashSet::new(),
        }
    }

    pub fn print_schemas(&self) {
        for (name, (schema, _)) in &self.relations {
            println!("Schema for {}:", name);
            for field in schema.fields() {
                println!("    {}: {:?}", field.name(), field.data_type(),);
            }
        }
    }

    /// Add all parquet files in a directory as in-memory tables.
    pub async fn add_parquets(&mut self, directory: &Path, batch_size: usize) {
        let mut parquets = crate::util::read_parquets_in_dir(directory, batch_size);

        for (name, (path, schema, batches)) in parquets.drain() {
            // Register parquet (only the schema and path, the data is not loaded)
            // We will never use `ctx` to read the data (because the ctx api is async), we use our own `relations` map instead.
            // This is a workaround to make the schemas available to datafusion's session context.
            let _ = self
                .ctx
                .register_parquet(
                    &name,
                    path.as_path().to_str().unwrap(),
                    ParquetReadOptions::default().schema(schema.as_ref()),
                )
                .await;
            self.relations.insert(name, (schema, batches));
        }
    }

    /// Update internal state with contents of new plan .
    /// Must be done before running the plan!
    pub fn update_aliases(&mut self, plan: &Plan) {
        // Update catalog with aliases
        self.set_aliases(plan.aliases.clone());

        // Keep track of relations in the plan
        Self::find_relations_in_plan(&plan.root, &mut self.rels_in_plan);
    }

    /// Recursively find all relations in the plan (original name, not alias)
    fn find_relations_in_plan(node: &intermediate_plan::Node, rels: &mut HashSet<String>) {
        match node {
            intermediate_plan::Node::SequentialScan(s) => {
                rels.insert(s.relation.clone());
            }
            _ => {
                for child in node.children() {
                    Self::find_relations_in_plan(child, rels);
                }
            }
        }
    }

    /// Update self.alias_to_rel and self.rel_to_alias with the given aliases.
    fn set_aliases(&mut self, aliases: HashMap<String, String>) {
        self.alias_to_rel = aliases;

        // Update reverse mapping
        self.rel_to_alias = HashMap::new();
        for (alias, rel) in &self.alias_to_rel {
            self.rel_to_alias
                .entry(rel.clone())
                .or_insert_with(HashSet::new)
                .insert(alias.clone());
        }
    }

    pub fn schemaof(&self, relation: &str) -> Arc<Schema> {
        self.relations[relation].0.clone()
    }

    /// Get reference to the data of `relation` as a vector of [RecordBatch].
    pub fn get_data(&self, relation: &str) -> &Vec<RecordBatch> {
        &self
            .relations
            .get(relation)
            .expect(&format!(
                "Relation {} not found, valid relations: {:?}",
                relation,
                self.relations.keys().collect::<Vec<_>>()
            ))
            .1
        // &self.relations[relation].1
    }

    /// Get the index of a field in a relation.
    /// Relation must be a table name or an alias.
    pub fn idx_of_field(&self, relation_or_alias: &str, field: &str) -> usize {
        let relation_or_alias = &relation_or_alias.to_lowercase();
        // Resolve alias first
        if self.alias_to_rel.contains_key(relation_or_alias) {
            let table_name = &self.alias_to_rel[relation_or_alias];
            return self.idx_of_field_helper(table_name, field);
        }
        // Relation is a table name
        return self.idx_of_field_helper(relation_or_alias, field);
    }

    /// Get the index of a field in a relation.
    /// Relation name must be the original name (not an alias).
    fn idx_of_field_helper(&self, relation: &str, field: &str) -> usize {
        self.relations[relation]
            .0
            .index_of(field.to_lowercase().as_str())
            .expect(&format!("Field {} not found", field))
    }

    pub fn get_state(&self) -> SessionState {
        let ctx = &self.ctx;
        ctx.state()
    }

    /// Make a FROM clause from the catalog's tables and aliases.
    /// - Relations with aliases will be formatted as `"table as alias"`.
    /// - Relations without aliases will be formatted as `"table"`.
    ///
    /// e.g: `"table1 as t1, table2 as t2, table3"`
    pub fn build_from_clause(&self) -> String {
        self.relations
            .keys()
            .filter(|rel| self.rels_in_plan.contains(*rel)) // exclude relations not in the plan
            .map(|relation| match self.rel_to_alias.get(relation) {
                Some(aliases) => aliases
                    .iter()
                    .map(|alias| format!("{} as {}", relation, alias))
                    .collect::<Vec<_>>()
                    .join(", "),
                None => relation.to_string(),
            })
            .collect::<Vec<_>>()
            .join(", ")
    }

    pub fn aliases(&self) -> &HashMap<String, String> {
        &self.alias_to_rel
    }

    pub fn session_context(&self) -> &SessionContext {
        &self.ctx
    }

    /// Map an alias to its relation name.
    pub fn resolve_alias(&self, alias: &str) -> Option<&String> {
        self.alias_to_rel.get(alias)
    }
}
